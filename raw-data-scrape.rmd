# Set constants
```{r}
DIR = 'D:/Onedrive/__Projects/jasmine-project-futureperfect' # Set to directory of desired file export
```

# Load libraries
```{r}
library(tidyverse)
library(data.table)
library(lubridate)
library(httr)
library(tidytext)
```

# Scrape raw data from graph API
```{r}
local({
# FB Data 
access_token =
	'EABCtUs97lNIBAFvqiqoilExlNGPWVQyyxD6ON5vJnv7OGokvLFde5jCTtI1dqmG6NBmqegsx0fe1OD5d53MYg78bC2RVcsTzKVKUNJZAN1iQDrgxGJ1dPONZBt5ZCX4qcSZC65hcyRbIU9ISYOjVX0bK78NR2yYlQpt2GP7kAWsz2duR0n3kZC7EfVzSFHMs6ZBpo4fJd5PicZBgjBB1EqO0ethjnryXZCPJINJEis9V1AZDZD'


# https://stackoverflow.com/questions/36906590/getting-facebook-post-all-reactions-count-in-single-graph-api-request
# https://developers.facebook.com/docs/graph-api/reference/post/
# https://developers.facebook.com/docs/graph-api/reference/pagepost
pageData = list()
for (i in 1:30) {
	
	message('Getting posts ', (i - 1) * 100, ' to ', i * 100)
	
	if (i == 1) {
    	pageContent =
			GET(
			    'https://graph.facebook.com',
			    path = '/helperase/posts',
			    query = list(
			    	fields = '
			    	id,
			    	created_time,
			    	permalink_url,
			    	status_type,
			    	full_picture,
			    	icon,
			    	story,
			    	message_tags,
			    	message,
			    	shares,
			    	reactions.type(LIKE).limit(0).summary(total_count).as(like),
			    	reactions.type(LOVE).limit(0).summary(total_count).as(love),
			    	reactions.type(HAHA).limit(0).summary(total_count).as(haha),
			    	reactions.type(WOW).limit(0).summary(total_count).as(wow),
			    	reactions.type(SAD).limit(0).summary(total_count).as(sad),
			    	reactions.type(ANGRY).limit(0).summary(total_count).as(angry)
			    	',
			    	access_token = access_token,
			    	limit = 100
			    	)
			) %>% content(.) 
    } else {
    	pageContent = GET(nextPageUrl) %>% content(.)
	}
  
	
	pageData[[i]] =
		pageContent$data %>%
		lapply(., function(x)
			# Fix issue with nested lists for reactions
			x %>%
				purrr::imap(., function(z, i) {
					if('summary' %in% names(z)) z$summary$total_count
					else if ('count' %in% names(z)) z$count 
					else if (i == 'message_tags') map(z, ~ .$name) %>% paste0(., collapse = ' ')
					else z
					}) %>%
				as_tibble(.)
			) %>%
		dplyr::bind_rows(.)
	
	message(i, pageContent$paging)
	
	nextPageUrl = pageContent$paging$'next'
}



	emojiReplacements = {setNames(paste0(' [', names(emo::ji_name), '] '), emo::ji_name)} %>% .[1:3500]
	pageDf =
		dplyr::bind_rows(pageData) %>%
		# Convert unicode emojis to [emoji_description]
		dplyr::mutate(., message = str_replace_all(message, emojiReplacements)) %>%
		# Fix other weird characters
		dplyr::mutate(., message = str_replace_all(message, coll('’'), '\''), message = str_replace_all(message, coll('‘'), '\'')) %>%
		dplyr::mutate(., message = str_replace_all(message, coll('“'), '"'), message = str_replace_all(message, coll('”'), '"')) %>%
		dplyr::mutate(., message = str_replace_all(message, coll('–'), '-')) %>%
		dplyr::mutate(., message = str_replace_all(message, coll('—'), '-')) %>%
		dplyr::mutate(., message = str_replace_all(message, coll(' ￼'), ' ')) %>%
		# Strip remaining non-ASCII text completely
		dplyr::mutate(., message = iconv(message, from = 'utf-8', 'ASCII', sub = '')) %>%
		dplyr::mutate(., message = str_replace_all(message, coll('_'), '')) %>%
		# Replace linebreaks with spaces
		dplyr::mutate(., message = str_replace_all(message, coll('\n'), ' ')) %>%
		# Now parse dates into R date format and convert to Eastern time
    	dplyr::mutate(., created_time = parse_date_time2(created_time, '%Y-%m-%d%H:%M:%S+%z', tz = 'UTC') %>% with_tz(., 'America/New_York')) %>%
		# Now create 'engagements column' by summing all reactions + shares	
		dplyr::mutate(., shares = replace_na(shares, 0)) %>%
		dplyr::mutate(., engagements = shares + like + love + haha + wow + sad + angry)

	
})
```

# Export raw data file
```{r}
fwrite(pageDf, file.path(DIR, 'raw-data.csv'))
```



# Feature extraction
```{r}
local({
	
	sentimentsDf =
		get_sentiments('nrc') %>%
		dplyr::mutate(
			sentiment = ifelse(sentiment == 'disgust', 'negative', sentiment),
			sentiment = ifelse(sentiment == 'sadness', 'negative', sentiment),
			sentiment = ifelse(sentiment == 'joy', 'positive', sentiment),
			sentiment = ifelse(sentiment == 'trust', 'positive', sentiment),
			sentiment = ifelse(sentiment == 'surprise', 'neutral', sentiment),
			sentiment = ifelse(sentiment == 'anticipation', 'neutral', sentiment),
			sentiment = ifelse(sentiment == 'fear', 'negative', sentiment),
			sentiment = ifelse(sentiment == 'anger', 'negative', sentiment)
		)

	miscFeatures =
		pageDf %>%
		dplyr::arrange(., created_time) %>%
		dplyr::transmute(
			.,
			id,
			is_video = ifelse(status_type == 'added_video', 1, 0),
			is_story = ifelse(status_type == 'shared_story', 1, 0),
			is_mobile_status_update = ifelse(status_type == 'mobile_status_update', 1, 0),
			is_photo = ifelse(status_type == 'added_photos', 1, 0),
			created_weekday = lubridate::wday(created_time), # 1 = Sun, 7 = Sat
			is_created_morning = ifelse(lubridate::hour(created_time) %in% 5:10, 1, 0),
			is_created_afternoon = ifelse(lubridate::hour(created_time) %in% 11:16, 1, 0),
			is_created_evening = ifelse(lubridate::hour(created_time) %in% 17:20, 1, 0),
			is_created_night = ifelse(lubridate::hour(created_time) %in% c(21:24, 0:4), 1, 0),
			hours_since_last_post = as.numeric(difftime(created_time, dplyr::lag(created_time, 1), units = 'hours')), 
			engagements_last_post = dplyr::lag(engagements, 1)
			)
	
	
	data(stop_words)
	
	textSentimentFeatures =
		pageDf %>%
		dplyr::select(., id, message) %>%
		tidytext::unnest_tokens(., word, message) %>%
		dplyr::anti_join(., stop_words, by = 'word') %>%
		dplyr::inner_join(., sentimentsDf, by = 'word') %>%
		dplyr::left_join(
			.,
			dplyr::group_by(., id) %>%
				dplyr::summarize(., postTotal = n(), .groups = 'drop'),
			by = 'id'
		) %>%
		dplyr::group_by(., sentiment, id) %>%
		dplyr::summarize(., count = n(), postTotal = unique(postTotal), .groups = 'drop') %>%
		dplyr::mutate(., percent = count/postTotal) %>%
		# Get predominant emotion
		dplyr::group_by(., id) %>%
		dplyr::filter(., percent == max(percent)) %>%
		dplyr::summarize(., text_emotional_intensity = head(percent, 1), text_primary_emotion = head(sentiment, 1))
	
	textOtherFeatures =
		pageDf %>%
	    dplyr::transmute(
	        .,
	        id, 
	        text_has_hastag = ifelse(str_detect(message, coll('#')) == TRUE, 1, 0),
	        text_has_hastag = ifelse(is.na(text_has_hastag), 0, text_has_hastag),
			text_num_hashtags = ifelse(is.na(message_tags), 0, str_count(message, coll('#'))),
	        text_length = str_count(message, '\\S+'),
			text_length = ifelse(is.na(text_length), 0, text_length),
	        text_missing_notice = ifelse(str_detect(message, regex('#missing|#Missing|#pleaseshare|#PleaseShare|FoundSafe|Safe')) == TRUE, 1, 0),
			text_missing_notice = ifelse(is.na(text_missing_notice), 0, text_missing_notice)
	    )
	
	featuresDf =
		pageDf %>%
		dplyr::full_join(., miscFeatures, by = 'id') %>%
		dplyr::full_join(., textSentimentFeatures, by = 'id') %>%
		dplyr::full_join(., textOtherFeatures, by = 'id') %>%
		dplyr::mutate(
			.,
			text_emotional_intensity = ifelse(is.na(text_emotional_intensity), 0, text_emotional_intensity),
			text_primary_emotion = ifelse(is.na(text_primary_emotion), 0, text_primary_emotion)
		)
	
	
	featuresDf <<- featuresDf

})
```


# Features df
```{r}
fwrite(featuresDf, file.path(DIR, 'features-data.csv'))
```


# Some analysis
```{r}
local({
	
	
	reactionsChart =
		pageDf %>%
		dplyr::select(., id, engagements, created_time, love, haha, wow, sad, angry) %>%
		dplyr::mutate(., love = love, total_reaction_count = love + haha + wow + sad + angry) %>%
		dplyr::filter(., total_reaction_count > 0) %>%
		tidyr::pivot_longer(., -c('id', 'total_reaction_count', 'created_time', 'engagements'), names_to = 'emotion') %>%
		dplyr::mutate(., pct_reaction = value/total_reaction_count) %>%
		dplyr::group_by(id, total_reaction_count, created_time, engagements) %>%
		dplyr::filter(., pct_reaction == max(pct_reaction)) %>%
		dplyr::summarize(., top_emotion = head(emotion, 1), top_emotion_pct = head(pct_reaction, 1), .groups = 'drop') %>%
		dplyr::group_by(top_emotion) %>%
		dplyr::summarize(., median_engagements = mean(engagements), .groups = 'drop') %>%
		ggplot(.) +
		geom_bar(aes(x = top_emotion, y = median_engagements, fill = top_emotion), stat = 'identity') +
		labs(
			title = 'Median Engagements Per Post For Each Reaction Type',
			x = 'Top Reaction Type',
			y = 'Median Number of Engagements',
			fill = 'Primary Reaction Type'
			) +
		ggthemes::theme_fivethirtyeight()
	
	
	tsChart =
		featuresDf %>%
	    dplyr::mutate(., top_post = ifelse(engagements > 200, TRUE, FALSE)) %>%
	    purrr::transpose(.) %>%
	    purrr::imap(., function(row, i) {
	        if (row$top_post == FALSE) return(NA)
	        
	        featuresDf %>%
	            .[(i + 1):nrow(.),] %>%
	            dplyr::transmute(
	                .,
	                id,
	                engagements,
	                time_since_top_post = as.numeric(difftime(lubridate::as_datetime(row$created_time), created_time, units = 'days'))
	            ) %>%
	            dplyr::filter(., time_since_top_post < 60) %>%
	            return(.)
	    }) %>%
	    purrr::keep(., ~ !all(is.na(.))) %>%
	    dplyr::bind_rows(.) %>%
	    dplyr::mutate(., time_since_top_post = round(time_since_top_post * .5) / .5) %>%
	    dplyr::group_by(., time_since_top_post) %>%
	    dplyr::summarize(., median_engagements = mean(engagements), .groups = 'drop') %>%
	    ggplot(.) +
	    geom_line(aes(x = time_since_top_post, y = median_engagements), color = 'blue', size = 1) +
		geom_vline(xintercept = 0, color = 'red', size = 2) +
		geom_text(aes(x = 0, y = 60, label = 'Highly Successful Post'), color = 'red', angle = 90, vjust = -1.0) +
		scale_x_continuous(breaks = seq(0, 90, by = 7), minor_breaks = 0:90) +
		ggthemes::theme_solarized() +
		labs(
			title = 'Successful posts follow other successful posts!',
			x = 'Days Since Highly Successful Post',
			y = 'Mean Number of Engagements'
			)
	
	
# featuresDf %>% dplyr::transmute(., id, is_successful = ifelse(engagements > 100, '100+ Engagements', '<100 Engagements'), message) %>% 	tidytext::unnest_tokens(., word, message) %>%
#     dplyr::anti_join(., stop_words, by = 'word') %>% dplyr::group_by(., is_successful, word) %>% dplyr::summarize(., n = n(), .groups = 'drop') %>% dplyr::group_by(is_successful) %>% dplyr::mutate(., proportion = n/n()) %>% dplyr::arrange(desc(proportion)) %>% {with(wordcloud())}		featuresDf %>% dplyr::transmute(., id, engagements, message = message_tags) %>% na.omit(.) %>%	tidytext::unnest_tokens(., word, message) %>%
	hashtagChart =
		featuresDf %>%
		dplyr::transmute(., id, engagements, message = message_tags) %>%
		na.omit(.) %>%
		tidytext::unnest_tokens(., word, message) %>%
		dplyr::anti_join(., stop_words, by = 'word') %>%
		dplyr::mutate(., total_words = nrow(.)) %>%
		dplyr::group_by(word, total_words) %>% 
		dplyr::summarize(., word_count = n(), mean_engagements = median(engagements), .groups = 'drop') %>%
		dplyr::filter(., word_count > 2) %>%
		dplyr::mutate(., word_prop = word_count/total_words) %>%
		dplyr::mutate(., word = paste0('#', word)) %>%
		dplyr::arrange(., desc(mean_engagements)) %>%
		ggplot(.) +
		geom_point(aes(x = word_prop, y = mean_engagements), position = position_jitter(w = 0.03, h = 0), alpha = .1, size = 5.0, color = 'red') +
		geom_text(aes(x = word_prop, y = mean_engagements, label = word), position = position_jitter(w = .05, h = 0), size = 4.5, check_overlap = TRUE, color = 'black', alpha = .8, hjust = 'center', vjust = 'middle') +
		annotate('segment', x = .004, y = 10, xend = .1, yend = 10, arrow = arrow(length = unit(0.5, 'cm')), size = 2, alpha = .5, color = 'forestgreen') +
		geom_text(aes(x = .05, y = 13, label = 'More Frequently Used'), color = 'forestgreen') +
		annotate('segment', x = .004, y = 10, xend = .004, yend = 250, arrow = arrow(length = unit(0.5, 'cm')), size = 2, alpha = .5, color = 'blue') +
		geom_text(aes(x = .004, y = 200, label = 'Higher Engagement Rate'), color = 'blue', hjust = -.2) +
		scale_x_log10() +
		scale_y_log10() +
		geom_hline(yintercept = 10) +
		geom_vline(xintercept = .004) +
		labs(title = 'Which hastags are successful?', x = 'Word Frequency (Percent of All Words)', y = 'Median Engagement Rate')
	
	
	wordChart =
		featuresDf %>%
		dplyr::transmute(., id, engagements, message = message) %>%
		na.omit(.) %>%
		tidytext::unnest_tokens(., word, message, 'tweets') %>%
		dplyr::anti_join(., stop_words, by = 'word') %>%
		dplyr::mutate(., total_words = nrow(.)) %>%
		dplyr::group_by(word, total_words) %>% 
		dplyr::summarize(., word_count = n(), mean_engagements = median(engagements), .groups = 'drop') %>%
		dplyr::filter(., word_count > 30) %>%
		dplyr::mutate(., word_prop = word_count/total_words) %>%
		dplyr::mutate(., word = paste0(word)) %>%
		dplyr::arrange(., desc(mean_engagements)) %>%
		ggplot(.) +
		geom_point(aes(x = word_prop, y = mean_engagements), position = position_jitter(w = 0.03, h = 0), alpha = .1, size = 3.5, color = 'blue') +
		geom_text(aes(x = word_prop, y = mean_engagements, label = word), position = position_jitter(w = .05, h = 0), size = 4.5, check_overlap = TRUE, color = 'black', alpha = .8, hjust = 'center', vjust = 'middle') +
		scale_x_log10() +
		labs(title = 'Which words are successful?', subtitle = 'for words used 20+ times', x = 'Word Frequency (Percent of All Words)', y = 'Median Engagement Rate')
	
	
	chartsObj = list(words = wordChart, hashtags = hashtagChart, ts = tsChart, reactions = reactionsChart)
	chartsObj <<- chartsObj
})
```

```{r}
chartsObj %>%
	imap(., function(x, i)
		ggsave(plot = x, filename = file.path(DIR, paste0(i, '.png')), width = 8, height = 4.5, units = 'in', scale = 1.5)
		)

```



